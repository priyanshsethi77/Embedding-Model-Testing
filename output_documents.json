[
  {
    "id": 1,
    "file_name": "240809869v5.pdf",
    "text": "Docling Technical Report Version 1.0 Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar AI4K Group, IBM Research R¨uschlikon, Switzerland Abstract This technical report introduces Docling, an easy to use, self-contained, MIT- licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models. 1 Introduction Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial soft- ware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions. With Docling, we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recog- nition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hard- ware. Its code architecture allows for easy extensibility and addition of new features and models. Docling Technical Report 1 arXiv:2408.09869v5  [cs.CL]  9 Dec 2024 Here is what Docling delivers today: • Converts PDF documents to JSON or Markdown format, stable and lightning fast • Understands detailed page layout, reading order, locates figures and recovers table struc- tures • Extracts metadata from the document, such as title, authors, references and language • Optionally applies OCR, e.g. for scanned PDFs • Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution) • Can leverage different accelerators (GPU, MPS, etc). 2 Getting Started To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance. Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository. from docling. document_converter import DocumentConverter source = \"https :// arxiv.org/pdf /2206.01062\" # PDF path or URL converter = DocumentConverter () result = converter. convert_single (source) print(result. render_as_markdown ()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\" Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container. 3 Processing pipeline Docling implements a linear pipeline of operations, which execute sequentially on each given docu- ment (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown. 3.1 PDF backends Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling’s PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive 1see huggingface.co/ds4sd/docling-models/ 2 Layout Analysis Serialize as  JSON or Markdown {;} Parse  PDF pages Table  Structure OCR Model Pipeline Assemble results, Apply document  post-processing Figure 1: Sketch of Docling’s default processing pipeline. The inner part of the model pipeline is easily customizable and extensible. licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14]. We therefore decided to provide multiple backend choices, and additionally open-source a custom- built PDF parser, which is based on the low-level qpdf[4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium, which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings. 3.2 AI models As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models. Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks. Layout Analysis Model Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5]. The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables. Table Structure Recognition The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2]. 3 The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predic- tions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells. OCR Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular third- party OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page). We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements. 3.3 Assembly In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core. The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, cor- recting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request. 3.4 Extensibility Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract base- class (BaseModelPipeline) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline config- uration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements. Implementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly. 4 Performance In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1. If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery. Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and 4 torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report. Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads. CPU Thread budget native backend pypdfium backend TTS Pages/s Mem TTS Pages/s Mem Apple M3 Max (16 cores) 4 177 s 1.27 6.20 GB 103 s 2.18 2.56 GB 16 167 s 1.34 92 s 2.45 Intel(R) Xeon E5-2690 (16 cores) 4 375 s 0.60 6.16 GB 239 s 0.94 2.42 GB 16 244 s 0.92 143 s 1.57 5 Applications Thanks to the high-quality, richly structured document conversion achieved by Docling, its out- put qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application pat- terns, such as retrieval-augmented generation (RAG), we provide quackling, an open-source package which capitalizes on Docling’s feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIn- dex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant ben- efit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets. 6 Future work and contributions Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equation- recognition model, a code-recognition model and more. This will help improve the quality of con- version for specific types of content, as well as augment extracted document metadata with ad- ditional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too. We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review. The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report. References [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR, 2024. Version: 1.7.0. [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. La- zos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster 5 machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Pro- gramming Languages and Operating Systems, Volume 2 (ASPLOS ’24). ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf. [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD), pages 363–373. IEEE, 2022. [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf. [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/, 2024. Version: 1.18.1. [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit. [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF. [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index. [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´e, CA, USA, August 21–26, 2023, Proceedings, Part II, pages 37–50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3. [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. State- ments: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christi- aen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024), pages 193–214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15. [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications, 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y. [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4614–4623, 2022. [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large human- annotated dataset for document-layout segmentation. pages 3743–3751, 2022. [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf. [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2. [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023. 6 Appendix In this section, we illustrate a few examples of Docling’s output in Markdown and JSON. Birgit Pﬁtzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com Michele Dolﬁ IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground- truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientiﬁc article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops signiﬁcantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufﬁcient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis. · Information systems → Document structure ; · Applied computing → Document analysis ; · Computing methodologies → Machine learning ; Computer vision ; Object detection ; Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 Figure 1: Four examples of complex page layouts across different document categories PDF document conversion, layout segmentation, object-detection, data set, Machine Learning Birgit Pﬁtzmann, Christoph Auer, Michele Dolﬁ, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis ABSTRACT CCS CONCEPTS KEYWORDS ACM Reference Format: DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis Birgit P￿tzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com Michele Dol￿ IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com ABSTRACT Accurate document layout analysis is a key requirement for high- quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very e￿ective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scienti￿c article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops signi￿cantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Fur- thermore, we provide evidence that DocLayNet is of su￿cient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet- trained models are more robust and thus the preferred choice for general-purpose document-layout analysis. CCS CONCEPTS • Information systems →Document structure; • Applied com- puting →Document analysis; • Computing methodologies →Machine learning; Computer vision; Object detection; Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro￿t or commercial advantage and that copies bear this notice and the full citation on the ￿rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD ’22, August 14–18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043 13 USING THE VERTICAL TUBE - MODELS AY11230/11234 1. The vertical tube can be used for          instructional viewing or to photograph       the image with a digital camera or a        micro TV unit   2. Loosen the retention screw, then rotate        the adjustment ring to change the           length of the vertical tube. 3. Make sure that both the images in   OPERATION (cont.) SELECTING OBJECTIVE  MAGNIFICATION    1. There are two objectives. The lower       magnification objective has a greater       depth of field and view. 2. In order to observe the specimen       easily use the lower magnification       objective first. Then, by rotating the       case, the magnification can be        changed. CHANGING THE INTERPUPILLARY  DISTANCE 1. The distance between the observer's       pupils is the interpupillary distance.    2. To adjust the interpupillary distance       rotate the prism caps until both eyes       coincide with the image in the        eyepiece.    FOCUSING 1. Remove the lens protective cover. 2. Place the specimen on the working       stage. 3. Focus the specimen with the left eye       first while turning the focus knob until       the image appears clear and sharp. 4. Rotate the right eyepiece ring until the       images in each eyepiece coincide and       are sharp and clear. CHANGING THE BULB 1. Disconnect the power cord. 2. When the bulb is cool, remove the       oblique illuminator cap and remove       the halogen bulb with cap. 3. Replace with a new halogen bulb. 4. Open the window in the base plate and       replace the halogen lamp or         fluorescent lamp of transmitted        illuminator. FOCUSING 1. Turn the focusing knob away or toward       you until a clear image is viewed. 2. If the image is unclear, adjust the       height of the elevator up or down,       then turn the focusing knob again. ZOOM MAGNIFICATION 1. Turn the zoom magnification knob to       the desired magnification and field of       view. 2. In most situations, it is recommended       that you focus at the lowest         magnification, then move to a higher       magnification and re-focus as         necessary. 3. If the image is not clear to both eyes       at the same time, the diopter ring may       need adjustment. DIOPTER RING ADJUSTMENT 1. To adjust the eyepiece for viewing with       or without eyeglasses and for         differences in acuity between the right       and left eyes, follow the following       steps:     a. Observe an image through the left           eyepiece and bring a specific point           into focus using the focus knob.     b. By turning the diopter ring             adjustment for the left eyepiece,           bring the same point into sharp           focus.      c.Then bring the same point into           focus through the right eyepiece            by turning the right diopter ring.      d.With more than one viewer, each           viewer should note their own            diopter ring position for the left            and right eyepieces, then before           viewing set the diopter ring            adjustments to that setting. CHANGING THE BULB 1. Disconnect the power cord from the       electrical outlet. 2. When the bulb is cool, remove the       oblique illuminator cap and remove       the halogen bulb with cap. 3. Replace with a new halogen bulb. 4. Open the window in the base plate        and replace the halogen lamp or       fluorescent lamp of transmitted        illuminator.           Model AY11230 Model AY11234 14 Objectives Revolving Turret Coarse  Adjustment Knob MODEL AY11236 MICROSCOPE USAGE BARSKA Model AY11236 is a powerful fixed power compound  microscope designed for biological studies such as specimen  examination. It can also be used for examining bacteria and           for general clinical and medical studies and other scientific uses.  CONSTRUCTION BARSKA Model AY11236 is a fixed power compound microscope.    It is constructed with two optical paths at the same angle. It is  equipped with transmitted illumination. By using this instrument,  the user can observe specimens at magnification from 40x to  1000x by selecting the desired objective lens. Coarse and fine  focus adjustments provide accuracy and image detail. The rotating  head allows the user to position the eyepieces for maximum  viewing comfort and easy access to all adjustment knobs.   Model AY11236 Fine  Adjustment Knob Stage Condenser  Focusing Knob Eyepiece Stand Lamp  On/Off Switch Lamp  Power Cord Rotating Head Stage Clip Adjustment Interpupillary Slide Adjustment Circling Minimums 7KHUH\u0003ZDV\u0003D\u0003FKDQJH\u0003WR\u0003WKH\u00037(536\u0003FULWHULD\u0003LQ\u0003\u0015\u0013\u0014\u0015\u0003WKDW\u0003DႇHFWV\u0003FLUFOLQJ\u0003DUHD\u0003GLPHQVLRQ\u0003E\\\u0003H[SDQGLQJ\u0003WKH\u0003DUHDV\u0003WR\u0003SURYLGH\u0003 improved obstacle protection. To indicate that the new criteria had been applied to a given procedure, a   is placed on  the circling line of minimums. The new circling tables and explanatory information is located in the Legend of the TPP. 7KH\u0003DSSURDFKHV\u0003XVLQJ\u0003VWDQGDUG\u0003FLUFOLQJ\u0003DSSURDFK\u0003DUHDV\u0003FDQ\u0003EH\u0003LGHQWL¿HG\u0003E\\\u0003WKH\u0003DEVHQFH\u0003RI\u0003WKH\u0003  on the circling line of  minima. $SSO\\\u00036WDQGDUG\u0003&LUFOLQJ\u0003$SSURDFK\u00030DQHXYHULQJ\u00035DGLXV\u00037DEOH $SSO\\\u0003([SDQGHG\u0003&LUFOLQJ\u0003$SSURDFK\u00030DQHXYHULQJ\u0003$LUVSDFH\u00035DGLXV\u0003 Table AIRPORT SKETCH                                                                                                                                  The airport sketch is a depiction of the airport with emphasis on runway pattern and related  information, positioned in either the lower left or lower right corner of the chart to aid pi- lot recognition of the airport from the air and to provide some information to aid on ground  navigation of the airport. The runways are drawn to scale and oriented to true north. Runway  dimensions (length and width) are shown for all active runways. Runway(s) are depicted based on what type and construction of the runway. Hard Surface Other Than  Hard Surface Metal Surface Closed Runway Under Construction Stopways,  Taxiways, Park- ing Areas Displaced  Threshold Closed   Pavement Water Runway Taxiways and aprons are shaded grey. Other runway features that may be shown are runway numbers, runway dimen- sions, runway slope, arresting gear, and displaced threshold. 2WKHU\u0003LQIRUPDWLRQ\u0003FRQFHUQLQJ\u0003OLJKWLQJ\u000f\u0003¿QDO\u0003DSSURDFK\u0003EHDULQJV\u000f\u0003DLUSRUW\u0003EHDFRQ\u000f\u0003REVWDFOHV\u000f\u0003FRQWURO\u0003WRZHU\u000f\u00031$9$,'V\u000f\u0003KHOL- pads may also be shown. $LUSRUW\u0003(OHYDWLRQ\u0003DQG\u00037RXFKGRZQ\u0003=RQH\u0003(OHYDWLRQ The airport elevation is shown enclosed within a box in the upper left corner of the sketch box and the touchdown zone  elevation (TDZE) is shown in the upper right corner of the sketch box. The airport elevation is the highest point of an  DLUSRUW¶V\u0003XVDEOH\u0003UXQZD\\V\u0003PHDVXUHG\u0003LQ\u0003IHHW\u0003IURP\u0003PHDQ\u0003VHD\u0003OHYHO\u0011\u00037KH\u00037'=(\u0003LV\u0003WKH\u0003KLJKHVW\u0003HOHYDWLRQ\u0003LQ\u0003WKH\u0003¿UVW\u0003\u0016\u000f\u0013\u0013\u0013\u0003IHHW\u0003RI\u0003 the landing surface. Circling only approaches will not show a TDZE. 114 FAA Chart Users’ Guide - Terminal Procedures Publication (TPP) - Terms AGL 2013 Financial Calendar 22 August 2012  2012 full year result and ﬁnal dividend announced 30 August 2012  Ex-dividend trading commences 5 September 2012  Record date for 2012 ﬁnal dividend 27 September 2012  Final dividend payable 23 October 2012  Annual General Meeting 27 February 20131  2013 interim result and interim dividend announced 28 August 20131  2013 full year results and ﬁnal dividend announced  1 Indicative dates only, subject to change/Board conﬁrmation AGL’s Annual General Meeting will be held at the City Recital Hall, Angel Place, Sydney  commencing at 10.30am on Tuesday 23 October 2012. Yesterday Established in Sydney in 1837, and then  known as The Australian Gas Light Company,  the AGL business has an established history  and reputation for serving the gas and  electricity needs of Australian households.  In 1841, when AGL supplied the gas to light  the ﬁrst public street lamp, it was reported  in the Sydney Gazette as a “wonderful  achievement of scientiﬁc knowledge, assisted  by mechanical ingenuity.” Within two years,  165 gas lamps were lighting the City of Sydney. Looking back on  175 years of  looking forward. AGL Energy Limited ABN 74 115 061 375 29 signs, signals and road markings 3 In chapter 2, you and your vehicle, you learned about  some of the controls in your vehicle. This chapter is a handy  reference section that gives examples of the most common  signs, signals and road markings that keep trafﬁc organized  and ﬂowing smoothly.  Signs There are three ways to read signs: by their shape, colour and  the messages printed on them. Understanding these three ways  of classifying signs will help you ﬁgure out the meaning of signs  that are new to you.  Stop Yield the right-of-way Shows driving   regulations Explains lane use School zone signs  are ﬂuorescent  yellow-green Tells about motorist  services Shows a permitted  action Shows an action that  is not permitted Warns of hazards  ahead Warns of   construction zones Railway crossing Shows distance and  direction • Signs  – regulatory signs  – school,  playground and  crosswalk signs  – lane use signs  –  turn control signs  –  parking signs  –  reserved lane  signs  –  warning signs  –  object markers  –  construction  signs  – information and  destination signs  –  railway signs • Signals  – lane control  signals  – trafﬁc lights • Road markings  – yellow lines  – white lines  – reserved lane  markings  – other markings in this chapter Figure 1: Four examples of complex page layouts across dif- ferent document categories KEYWORDS PDF document conversion, layout segmentation, object-detection, data set, Machine Learning ACM Reference Format: Birgit P￿tzmann, Christoph Auer, Michele Dol￿, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for Document- Layout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Wash- ington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043 arXiv:2206.01062v1  [cs.CV]  2 Jun 2022 Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown). 7 KDD ’22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default con￿gurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised us- ing pre-trained weights from the COCO 2017 dataset. human MRCNN FRCNN YOLO R50 R101 R101 v5x6 Caption 84-89 68.4 71.5 70.1 77.7 Footnote 83-91 70.9 71.8 73.7 77.2 Formula 83-85 60.1 63.4 63.5 66.2 List-item 87-88 81.2 80.8 81.0 86.2 Page-footer 93-94 61.6 59.3 58.9 61.1 Page-header 85-89 71.9 70.0 72.0 67.9 Picture 69-71 71.7 72.7 72.0 77.1 Section-header 83-84 67.6 69.3 68.4 74.6 Table 77-81 82.2 82.9 82.2 86.3 Text 84-86 84.6 85.8 85.4 88.1 Title 60-72 76.7 80.4 79.9 82.7 All 82-83 72.4 73.5 73.4 76.8 to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we in- troduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and e￿ort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture. For the latter, we instructed annotation sta￿to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to ￿ag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the ￿nal dataset. With all these measures in place, experienced annotation sta￿managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity. 5 EXPERIMENTS The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increas- ing fractions of the DocLayNet dataset. The learning curve ￿attens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield signi￿cantly better predictions. paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work. In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16]. Baselines for Object Detection In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025⇥1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text, Table and Picture. This is not entirely surprising, as Text, Table and Picture are abundant and the most visually distinctive in a document. Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in ”5. Experiments” wrapping over the column end is broken up in two and interrupted by the table. 8 % of Total % of Total % of Total triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) triple inter- annotator mAP @ 0.5-0.95 (%) class label Count Train Test Val All Fin Man Sci Law Pat Ten Caption 22524 2.04 1.77 2.32 84-89 40-61 86-92 94-99 95-99 69-78 n/a Footnote 6318 0.60 0.31 0.58 83-91 n/a 100 62-88 85-94 n/a 82-97 Formula 25027 2.25 1.90 2.96 83-85 n/a n/a 84-87 86-96 n/a n/a List-item 185660 17.19 13.34 15.82 87-88 74-83 90-92 97-97 81-85 75-88 93-95 Page- footer 70878 6.51 5.58 6.00 93-94 88-90 95-96 100 92-97 100 96-98 Page- header 58022 5.10 6.70 5.06 85-89 66-76 90-94 98-100 91-92 97-99 81-86 Picture 45976 4.21 2.78 5.31 69-71 56-59 82-86 69-82 80-95 66-71 59-76 Section- header 142884 12.60 15.77 12.85 83-84 76-81 90-92 94-95 87-94 69-73 78-86 Table 34733 3.20 2.27 3.60 77-81 75-80 83-86 98-99 58-80 79-84 70-85 Text 510377 45.82 49.28 45.00 84-86 81-86 88-93 89-93 87-92 71-79 87-95 Title 5071 0.47 0.30 0.50 60-72 24-63 50-63 94-100 82-96 68-79 24-56 Total 1107470 941123 99816 66531 82-83 71-74 79-81 89-94 86-91 71-76 68-85 % of Total triple inter-annotator mAP @ 0.5-0.95 (%) class label Count Train Test Val All Fin Man Sci Law Pat Ten Caption 22524 2.04 1.77 2.32 84-89 40-61 86-92 94-99 95-99 69-78 n/a Footnote 6318 0.60 0.31 0.58 83-91 n/a 100 62-88 85-94 n/a 82-97 Formula 25027 2.25 1.90 2.96 83-85 n/a n/a 84-87 86-96 n/a n/a List-item 185660 17.19 13.34 15.82 87-88 74-83 90-92 97-97 81-85 75-88 93-95 Page-footer 70878 6.51 5.58 6.00 93-94 88-90 95-96 100 92-97 100 96-98 Page-header 58022 5.10 6.70 5.06 85-89 66-76 90-94 98-100 91-92 97-99 81-86 Picture 45976 4.21 2.78 5.31 69-71 56-59 82-86 69-82 80-95 66-71 59-76 Section-header 142884 12.60 15.77 12.85 83-84 76-81 90-92 94-95 87-94 69-73 78-86 Table 34733 3.20 2.27 3.60 77-81 75-80 83-86 98-99 58-80 79-84 70-85 Text 510377 45.82 49.28 45.00 84-86 81-86 88-93 89-93 87-92 71-79 87-95 Title 5071 0.47 0.30 0.50 60-72 24-63 50-63 94-100 82-96 68-79 24-56 Total 1107470 941123 99816 66531 82-83 71-74 79-81 89-94 86-91 71-76 68-85 A B C Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header ”triple inter- annotator mAP@0.5-0.95 (%)”, is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C). 9",
    "tables": [],
    "images": [
      "240809869v5_page1_img1.jpeg",
      "240809869v5_page3_img1.png",
      "240809869v5_page3_img2.png",
      "240809869v5_page3_img3.png",
      "240809869v5_page3_img4.png",
      "240809869v5_page3_img5.png",
      "240809869v5_page3_img6.png",
      "240809869v5_page3_img7.png",
      "240809869v5_page3_img8.png",
      "240809869v5_page3_img9.png",
      "240809869v5_page3_img10.png",
      "240809869v5_page3_img11.png",
      "240809869v5_page3_img12.png",
      "240809869v5_page3_img13.png",
      "240809869v5_page3_img14.png",
      "240809869v5_page3_img15.png",
      "240809869v5_page7_img1.jpeg",
      "240809869v5_page7_img2.png",
      "240809869v5_page7_img3.png",
      "240809869v5_page7_img4.png",
      "240809869v5_page7_img5.png",
      "240809869v5_page8_img1.png",
      "240809869v5_page9_img1.png",
      "240809869v5_page9_img2.jpeg",
      "240809869v5_page9_img3.jpeg"
    ]
  },
  {
    "id": 2,
    "file_name": "document_1.pdf",
    "text": "The Eiffel Tower, known in French as La Tour Eiffel, is one of the most iconic landmarks in the world and a symbol of France's architectural innovation and cultural heritage. Located in the heart of Paris on the Champ de Mars, near the serene flow of the Seine River, the tower has stood tall as a representation of modern engineering and timeless elegance for over a century. Constructed between 1887 and 1889, the Eiffel Tower was originally designed as the entrance arch to the 1889 Exposition Universelle (World’s Fair), which marked the 100th anniversary of the French Revolution. The French engineer Gustave Eiffel, whose company designed and built the tower, became the namesake of the structure, though the actual design was the work of Maurice Koechlin and Émile Nouguier. At the time of its construction, the tower was a marvel of modern engineering, showcasing the possibilities of wrought iron and innovative design principles that defied traditional aesthetics. Despite its groundbreaking structure, the Eiffel Tower was met with strong criticism when it was first unveiled. Many prominent artists and intellectuals of the period considered it an eyesore and feared it would ruin the beauty of Paris. A group of influential Parisians even signed a petition calling for its dismantling. However, public opinion gradually shifted, and the tower’s appeal grew over time. What was once considered controversial became a beloved national treasure and an internationally recognized symbol of Paris. Standing at approximately 300 meters (about 984 feet), the Eiffel Tower was the tallest man-made structure in the world until the completion of the Chrysler Building in New York in 1930. The tower has three levels accessible to the public, with restaurants, observation decks, and gift shops offering various experiences. From the top platform, visitors can enjoy a panoramic view of Paris, including sights such as the River Seine, Notre-Dame Cathedral, and the distant Sacré-Cœur Basilica. One of the most magical aspects of the Eiffel Tower is its transformation after sunset. Each evening, the tower is illuminated with golden lights, and for the first five minutes of every hour, it sparkles with thousands of strobe lights, creating a breathtaking display that captivates both locals and tourists alike. This nightly light show has become one of the tower’s most beloved features and further reinforces its place as a romantic symbol of Paris. Today, the Eiffel Tower attracts nearly 7 million visitors annually from all corners of the globe, making it one of the most visited paid monuments in the world. It has been featured in countless films, books, and photographs, symbolizing everything from love and adventure to resilience and human achievement. Beyond tourism, the tower has also served practical purposes in telecommunications, with radio and television antennas mounted at the top. In conclusion, the Eiffel Tower is much more than a steel structure. It is a living piece of history, a product of visionary engineering, and a timeless icon that embodies the spirit of Paris. From its controversial beginnings to its current status as a global landmark, the Eiffel Tower continues to inspire wonder, pride, and admiration in all who see it.",
    "tables": [],
    "images": []
  },
  {
    "id": 3,
    "file_name": "document_2.pdf",
    "text": "Quantum computing is a groundbreaking area at the intersection of computer science and quantum physics, offering the potential to solve problems far beyond the capabilities of even the most powerful classical supercomputers. Traditional computers rely on classical bits, which store data as either a 0 or a 1. In contrast, quantum computers use quantum bits, or qubits, which can exist in a combination of both states simultaneously thanks to a fundamental quantum principle known as superposition. This concept of superposition allows a quantum computer to process a vast number of possibilities at once. But that's not the only unique aspect of qubits. Quantum computers also exploit entanglement, another quantum phenomenon where qubits become linked in such a way that the state of one instantly influences the state of another, even across long distances. This interdependence of qubits enables complex computations to be performed more efficiently than classical methods. Because of these principles, quantum computers have the potential to exponentially speed up certain types of computations. Problems that are computationally infeasible for classical systems—such as factoring very large numbers (important in cryptography), simulating molecular structures (critical in drug discovery), or optimizing complex systems (like traffic flow or financial modeling)—could be tackled much more efficiently using quantum algorithms. One well-known example is Shor’s algorithm, which can factor large integers exponentially faster than the best-known classical algorithms. If implemented on a sufficiently powerful quantum computer, this could render many current encryption systems (like RSA) obsolete. Another is Grover’s algorithm, which provides a quadratic speedup for searching unsorted databases, a common operation in computing. Despite the promising advantages, quantum computing is still in its early developmental stages. Building and maintaining a stable quantum computer is incredibly challenging due to the fragile nature of quantum states. Qubits are extremely sensitive to external interference such as heat or electromagnetic radiation, a problem known as quantum decoherence, which can cause errors in computation. Scientists and engineers are actively developing quantum error correction methods and more robust hardware designs to overcome these issues. Several technology giants—such as IBM, Google, Microsoft, and Intel—as well as startups and academic institutions are investing heavily in quantum research. Some companies have already made prototype quantum computers accessible via the cloud, allowing researchers to experiment with real quantum processors. Governments around the world, recognizing its strategic importance, are also funding national quantum initiatives. Quantum computing will not replace classical computers for all tasks. Instead, it will likely become a co-processor, used alongside classical systems for specialized applications. As the technology matures, it has the potential to revolutionize industries such as pharmaceuticals, finance, cybersecurity, artificial intelligence, and logistics by solving problems that are currently unsolvable. In conclusion, quantum computing represents a transformative leap in how we process information. By harnessing the strange but powerful laws of quantum mechanics, it opens the door to a new computing paradigm—one that could redefine what's computationally possible in the decades ahead.",
    "tables": [],
    "images": []
  },
  {
    "id": 4,
    "file_name": "document_3.pdf",
    "text": "The Mona Lisa, also known as La Gioconda, stands as one of the most recognizable and revered works of art in the world. Painted by the Italian Renaissance master Leonardo da Vinci between 1503 and 1506 (with some scholars suggesting work continued as late as 1517), the painting is renowned not only for its artistic brilliance but also for the mystique surrounding its subject, history, and global impact. At first glance, the Mona Lisa appears to be a simple portrait of a woman seated before a hazy landscape. But as any art historian or museum-goer will attest, the painting's deceptive simplicity conceals a profound depth of technique, emotion, and intrigue. The subject of the portrait is widely believed to be Lisa Gherardini, a Florentine woman and wife of wealthy merchant Francesco del Giocondo. This association is the origin of the painting's Italian name, La Gioconda. Yet, despite this general consensus, there remains debate about her true identity. Over the centuries, theories have emerged ranging from the idea that she was Leonardo’s mother to the claim that the painting is a disguised self- portrait of the artist himself. What makes the Mona Lisa especially captivating is her expression—often described as enigmatic. Her smile has been the subject of countless interpretations: is it one of contentment, melancholy, mystery, or amusement? Leonardo masterfully employed a technique known as sfumato, which involves the delicate blending of tones and colors without harsh outlines. This creates a soft, shadowy transition between colors and contributes significantly to the subject’s lifelike yet elusive demeanor. The sfumato technique is most evident in the subtle shaping of her face and eyes, and in the soft contours of her lips, giving her that legendary 'smile' that seems to change depending on the viewer's perspective. Beyond artistic technique, the Mona Lisa is significant as a symbol of Renaissance humanism. During the Renaissance, there was a renewed interest in human anatomy, naturalism, and individual personality, all of which are captured in the portrait. Unlike many religious or allegorical works of the time, the Mona Lisa focuses on a real human being—emphasizing personality, expression, and the beauty of the natural world, visible in the intricate background landscape. Today, the Mona Lisa resides in the Louvre Museum in Paris, France, where it draws over 10 million visitors annually. Her fame, however, is not solely based on her artistic merits. The painting gained worldwide notoriety in 1911 when it was stolen from the Louvre by an Italian handyman named Vincenzo Peruggia. His motive was nationalistic—he believed the painting rightfully belonged in Italy. The Mona Lisa remained missing for over two years before being recovered in Florence. Ironically, this act of theft elevated the painting’s status, transforming it from a revered artwork to a household name and global icon. Since then, the Mona Lisa has been the subject of countless studies, reproductions, parodies, and cultural references. From being featured in films and literature to being reinterpreted by modern artists like Marcel Duchamp and Banksy, the painting has transcended the boundaries of fine art to become a cultural phenomenon. Due to its immense value, both artistically and financially, the painting is now displayed under high-security measures. It sits behind bulletproof glass and is constantly monitored, ensuring its protection from theft, vandalism, or environmental damage. The Louvre has dedicated an entire room to this single painting, and it's not uncommon to see crowds gathered in front of it, all hoping to catch a glimpse or snap a photo of the famous smile. What makes the Mona Lisa truly remarkable is its timelessness. More than 500 years after its creation, it continues to spark curiosity, debate, and admiration. Art lovers appreciate the precision of Leonardo’s technique, historians delve into its rich backstory, psychologists analyze its emotional impact, and tourists simply marvel at its fame. In conclusion, the Mona Lisa is far more than a portrait. It is a masterful convergence of art, mystery, emotion, and history. Leonardo da Vinci, through his genius and vision, created a painting that has not only stood the test of time but has grown in significance over centuries. Whether studied in depth by scholars or admired briefly by travelers, the Mona Lisa embodies the very essence of art’s power to capture the imagination of generations across the globe.",
    "tables": [],
    "images": []
  },
  {
    "id": 5,
    "file_name": "document_4.pdf",
    "text": "Python is a dynamic, high-level, and interpreted programming language that has grown into one of the most widely used languages across industries and domains. Originally created in the late 1980s by Guido van Rossum and officially released in 1991, Python was designed to emphasize code readability and simplicity. Over the years, it has developed into a general-purpose language with applications ranging from web development and data analysis to machine learning, artificial intelligence, automation, and more. One of the defining features of Python is its clean and easy-to-understand syntax. Compared to languages like C++ or Java, Python code is more concise and closer to human language, which makes it particularly attractive for beginners. At the same time, it is powerful and flexible enough to be used by experienced developers for complex tasks. Its syntax enforces good programming practices and reduces the cognitive load on developers, enabling faster development and easier debugging. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. This flexibility allows developers to choose the style that best fits their needs. For instance, small scripts can be written procedurally, large systems can be designed with object-oriented principles, and data-processing tasks can be handled using functional tools like map, filter, and lambda expressions. This makes Python a go-to language for various project sizes and types. A major strength of Python lies in its massive ecosystem of libraries and frameworks. The Python Package Index (PyPI) hosts hundreds of thousands of third-party packages that extend the language's capabilities. In scientific computing, libraries such as NumPy and SciPy enable fast numerical operations and mathematical modeling. For data manipulation and analysis, Pandas provides robust tools that are now standard in the data science workflow. Python has also emerged as a leader in machine learning and artificial intelligence. Popular frameworks such as TensorFlow, PyTorch, and scikit-learn are written in or support Python, allowing researchers and engineers to build and train complex models with relative ease. Python’s integration with GPU-based computing also makes it possible to handle massive datasets and train deep learning networks efficiently. Web development is another area where Python excels. Frameworks like Flask, a lightweight microframework, and Django, a high- level framework that promotes rapid development, are widely used to create everything from simple APIs to enterprise-grade web applications. These frameworks come with extensive documentation and community support, making it easier for developers to launch secure and scalable applications quickly. Another reason behind Python’s popularity is its use in scripting and automation. From automating file operations and interacting with APIs to managing databases and scraping websites, Python provides the tools to build efficient and maintainable automation workflows. This makes it incredibly useful for DevOps professionals and system administrators. Python’s role in education should also be acknowledged. Due to its simplicity, many academic institutions have adopted Python as the introductory language for computer science courses. Its readability allows students to grasp fundamental programming concepts without being overwhelmed by complex syntax, helping them build a strong foundation. Community is another pillar of Python's success. The Python community is vast, welcoming, and highly active. With numerous forums, conferences (like PyCon), and open-source projects, developers from all levels of experience can find support, resources, and opportunities to collaborate. Python’s official documentation is also detailed and beginner-friendly, which contributes to its learning curve being smoother than that of many other languages. Python’s compatibility with other languages and systems also enhances its utility. Through interfaces like Cython, Jython, or IronPython, Python code can be integrated with C/C++, Java, and .NET respectively. This interoperability enables developers to leverage Python’s simplicity while still tapping into the performance or features of other languages. Python’s open-source nature ensures that it remains free to use and modify. It is supported across all major platforms, including Windows, macOS, and Linux. Continuous contributions from the open-source community and active development under the guidance of the Python Software Foundation (PSF) ensure that the language evolves while maintaining backward compatibility. In recent years, Python has even found its way into mobile development (via Kivy), game development (via Pygame), and blockchain applications. It is also widely used in robotics, IoT (Internet of Things), cybersecurity, and bioinformatics—further proving its incredible versatility. In conclusion, Python’s rise to dominance in the programming world can be attributed to its simplicity, power, and flexibility. Its wide-ranging libraries and frameworks, cross-domain usability, and supportive community have made it the first choice for developers, data scientists, researchers, educators, and businesses alike. Whether you're building a basic web application, conducting high-level AI research, or automating daily tasks, Python continues to be a dependable and forward-looking language that empowers innovation across disciplines.",
    "tables": [],
    "images": []
  },
  {
    "id": 6,
    "file_name": "document_5.pdf",
    "text": "The Great Wall of China is one of the most iconic and enduring architectural marvels in human history. Stretching across more than 13,000 miles of northern China, it stands as a testament to the vision, determination, and engineering prowess of ancient Chinese civilizations. The wall, often shrouded in myth and legend, was built over several centuries and dynasties, with its primary purpose being defense against nomadic invaders and military incursions. It remains not only a symbol of China's strength and perseverance but also a living piece of history that draws the fascination of people from around the globe. The origins of the Great Wall date back to as early as the 7th century BC during the Warring States period. During this time, several feudal states constructed individual walls and fortifications to defend their territories from rival states and external threats. However, it was during the reign of Emperor Qin Shi Huang, the first emperor of a unified China in the 3rd century BC, that the idea of connecting and expanding these regional walls into a unified defense system began to take shape. The emperor ordered the linking of various walls and the construction of new segments using forced labor—comprising soldiers, peasants, and prisoners—under harsh and grueling conditions. Over the next millennium, successive dynasties continued to repair, extend, and reinforce the wall. The most extensive reconstruction took place during the Ming dynasty (1368–1644), whose rulers sought to fortify China's borders against the Mongol tribes of the north. The Ming emperors employed more advanced building materials such as bricks and stone in place of the earlier use of tamped earth and wood. They also introduced sophisticated designs, including watchtowers, signal beacons, barracks, and strategic gates that allowed for effective military coordination and communication across vast distances. The Great Wall is often mistakenly thought to be a single, continuous wall. In reality, it is a collection of walls and fortifications built in various regions, following the contours of mountains, valleys, deserts, and plateaus. The terrain it crosses is as diverse as it is rugged, ranging from the towering peaks of northern China to the windswept plains of the Gobi Desert. In many places, the wall snakes across dramatic ridgelines, offering breathtaking views and demonstrating the incredible effort required to build in such challenging conditions. One of the most enduring myths about the Great Wall is that it is visible from outer space with the naked eye. This claim, while widely circulated, is false. Astronauts have confirmed that the wall is difficult, if not impossible, to see without the aid of telescopic lenses, due to its narrow width and the way it blends into the natural landscape. Nonetheless, its cultural and historical significance is undeniable. Throughout its long history, the wall served multiple purposes. Primarily, it acted as a military defense mechanism to repel invasions. The structure’s watchtowers and signal systems allowed troops to communicate threats using smoke signals during the day and fire beacons at night. It also served as a psychological barrier—demonstrating the power and organizational capacity of the Chinese state—and as a means of regulating trade and immigration along the Silk Road. In some periods, it also functioned as a customs checkpoint to control goods and people entering or leaving the empire. Today, the Great Wall stands as a UNESCO World Heritage Site and is considered one of the New Seven Wonders of the World. It attracts millions of tourists each year who come from all corners of the globe to hike its ancient paths, take in panoramic views, and reflect on the immense human labor that went into its creation. Some of the most visited sections of the wall include Badaling, Mutianyu, Jinshanling, and Simatai—each offering unique experiences and levels of accessibility for travelers. Conservation efforts are ongoing, as many parts of the wall have deteriorated due to natural erosion, human vandalism, and neglect. Some remote sections are crumbling or have been reclaimed by nature, while others are being restored to preserve their structural integrity and historical value. These preservation initiatives are supported by both the Chinese government and international organizations dedicated to safeguarding global cultural heritage. In conclusion, the Great Wall of China is far more than just an ancient fortification. It is a symbol of China's enduring civilization, a remarkable feat of architecture and engineering, and a reflection of centuries of conflict, cooperation, and cultural identity. Its legacy continues to inspire awe, reminding us of humanity's capacity to dream, build, and protect across generations.",
    "tables": [],
    "images": []
  }
]